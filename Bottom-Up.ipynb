{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from examinebugs import examine_rev\n",
    "\n",
    "distmodels = [\"BasicDistanceModel\", \"GitDiffDistModel\", \n",
    "              \"MossDistModel\", \"PatientDiffDistModel\"]\n",
    "scoremodels = [\"SimpleScoreModel\", \"LongestScoreModel\", \n",
    "               \"ShortestScoreModel\", \n",
    "               \"OutEdgesScoreModel\", \"SizeScoreModel\", \n",
    "               \"TimeWeightedScoreModel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n",
      "Read patch models\n"
     ]
    }
   ],
   "source": [
    "buggy_dicts = {}\n",
    "for dm in distmodels:\n",
    "    for sm in scoremodels:\n",
    "        name = \"%s_%s\" % (sm, dm)\n",
    "        buggy_dicts[name] =  examine_rev(\n",
    "            '/home/cat/rzou/linux', 'mm', 'mm', \n",
    "            dm, sm,\n",
    "            '19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static struct page *no_page_table(struct vm_area_struct *vma,\n",
      "\t\tunsigned int flags)\n",
      "{\n",
      "\t/*\n",
      "\t * When core dumping an enormous anonymous area that nobody\n",
      "\t * has touched so far, we don't want to allocate unnecessary pages or\n",
      "\t * page tables.  Return error instead of NULL to skip handle_mm_fault,\n",
      "\t * then get_dump_page() will return NULL to leave a hole in the dump.\n",
      "\t * But we can only make this optimization where a hole would surely\n",
      "\t * be zero-filled if handle_mm_fault() actually did handle it.\n",
      "\t */\n",
      "\tif ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))\n",
      "\t\treturn ERR_PTR(-EFAULT);\n",
      "\treturn NULL;\n",
      "}\n",
      "\n",
      "static struct page *follow_page_pte(struct vm_area_struct *vma,\n",
      "\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n"
     ]
    }
   ],
   "source": [
    "for line in (eval(buggy_dict[674]['patch']['content'])):\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buggy_dict.keys()\n",
    "def change_name(name):\n",
    "    return name \\\n",
    "        .replace(\"SizeScoreModel_BasicDistanceModel\", \"SizeScoreModel\") \\\n",
    "        .replace(\"_\", \"\\_\") \\\n",
    "        .replace(\"Git\", \"Myer\") \\\n",
    "        .replace(\"ScoreModel\", \"\") \\\n",
    "        .replace(\"DistModel\", \"Dist\") \\\n",
    "        .replace(\"BasicDistanceModel\", \"UnitDist\") \\\n",
    "        .replace(\"TimeWeighted\", \"TimeWeightAvePath\") \\\n",
    "        .replace(\"Shortest\", \"MinPath\") \\\n",
    "        .replace(\"Longest\", \"MaxPath\") \\\n",
    "        .replace(\"Size\", \"PatchLen\") \\\n",
    "        .replace(\"Edges\", \"EdgesSum\") \\\n",
    "        .replace(\"Simple\", \"AvePath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPath\\_MyerDiffDist & 461.0000 & 38444.0000 & 323.0000 & 323.0000 & 0.0000 & 38444.0000 & 0.0000 & 11142.0000 \\\\\n",
      "AvePath\\_MyerDiffDist & 461.0000 & 38444.0000 & 323.0000 & 323.0000 & 0.0000 & 38444.0000 & 0.0000 & 11142.1429 \\\\\n",
      "AvePath\\_MossDist & 0.3350 & 0.7850 & 0.2450 & 0.2450 & 0.0000 & 0.7850 & 0.0000 & 0.3421 \\\\\n",
      "PatchLen\\_MyerDiffDist & 57.0000 & 24.0000 & 4.0000 & 18.0000 & 650.0000 & 650.0000 & 4.0000 & 201.0000 \\\\\n",
      "AvePath\\_PatientDiffDist & 461.0000 & 35998.0000 & 323.0000 & 323.0000 & 0.0000 & 35998.0000 & 0.0000 & 10443.2857 \\\\\n",
      "PatchLen & 57.0000 & 24.0000 & 4.0000 & 18.0000 & 650.0000 & 650.0000 & 4.0000 & 201.0000 \\\\\n",
      "TimeWeightAvePath\\_MossDist & 202608.0000 & 148176.0083 & 148176.0000 & 148176.0000 & 0.0000 & 202608.0000 & 0.0000 & 121392.0012 \\\\\n",
      "OutEdgesSum\\_MossDist & 0.2350 & 0.5400 & 0.2450 & 0.2450 & 0.0000 & 0.5400 & 0.0000 & 0.2579 \\\\\n",
      "OutEdgesSum\\_UnitDist & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 0.0000 & 1.0000 & 0.0000 & 0.0000 \\\\\n",
      "MinPath\\_MyerDiffDist & 461.0000 & 38444.0000 & 323.0000 & 323.0000 & 0.0000 & 38444.0000 & 0.0000 & 11142.0000 \\\\\n",
      "MinPath\\_PatientDiffDist & 461.0000 & 35998.0000 & 323.0000 & 323.0000 & 0.0000 & 35998.0000 & 0.0000 & 10443.0000 \\\\\n",
      "OutEdgesSum\\_MyerDiffDist & 369.0000 & 38121.0000 & 323.0000 & 323.0000 & 0.0000 & 38121.0000 & 0.0000 & 11036.0000 \\\\\n",
      "OutEdgesSum\\_PatientDiffDist & 369.0000 & 35675.0000 & 323.0000 & 323.0000 & 0.0000 & 35675.0000 & 0.0000 & 10337.0000 \\\\\n",
      "PatchLen\\_MossDist & 57.0000 & 24.0000 & 4.0000 & 18.0000 & 650.0000 & 650.0000 & 4.0000 & 201.0000 \\\\\n",
      "TimeWeightAvePath\\_MyerDiffDist & 278812800.0000 & 195350983.9283 & 195350400.0000 & 195350400.0000 & 0.0000 & 278812800.0000 & 0.0000 & 163382483.4183 \\\\\n",
      "TimeWeightAvePath\\_UnitDist & 1209600.0000 & 604800.0153 & 604800.0000 & 604800.0000 & 0.0000 & 1209600.0000 & 0.0000 & 604800.0022 \\\\\n",
      "MaxPath\\_UnitDist & 2.0000 & 2.0000 & 1.0000 & 1.0000 & 0.0000 & 2.0000 & 0.0000 & 1.0000 \\\\\n",
      "PatchLen\\_PatientDiffDist & 57.0000 & 24.0000 & 4.0000 & 18.0000 & 650.0000 & 650.0000 & 4.0000 & 201.0000 \\\\\n",
      "AvePath\\_UnitDist & 2.0000 & 2.0000 & 1.0000 & 1.0000 & 0.0000 & 2.0000 & 0.0000 & 1.1429 \\\\\n",
      "TimeWeightAvePath\\_PatientDiffDist & 278812800.0000 & 195350946.4610 & 195350400.0000 & 195350400.0000 & 0.0000 & 278812800.0000 & 0.0000 & 163382478.0659 \\\\\n",
      "MinPath\\_MossDist & 0.3350 & 0.7850 & 0.2450 & 0.2450 & 0.0000 & 0.7850 & 0.0000 & 0.3421 \\\\\n",
      "MaxPath\\_PatientDiffDist & 461.0000 & 35998.0000 & 323.0000 & 323.0000 & 0.0000 & 35998.0000 & 0.0000 & 10443.0000 \\\\\n",
      "MaxPath\\_MossDist & 0.3350 & 0.7850 & 0.2450 & 0.2450 & 0.0000 & 0.7850 & 0.0000 & 0.3421 \\\\\n",
      "MinPath\\_UnitDist & 2.0000 & 2.0000 & 1.0000 & 1.0000 & 0.0000 & 2.0000 & 0.0000 & 1.0000 \\\\\n"
     ]
    }
   ],
   "source": [
    "for name, buggy_dict in buggy_dicts.iteritems():\n",
    "    assert(buggy_dict.keys()[0] == 2984)\n",
    "    assert(buggy_dict.keys()[1]==17944)\n",
    "    percentiles = []\n",
    "    for pid, p in buggy_dict.iteritems():\n",
    "        percentiles.append(p['score'])\n",
    "    percentiles.append(max(percentiles))\n",
    "    percentiles.append(min(percentiles))\n",
    "    percentiles.append(sum(percentiles)/len(percentiles))\n",
    "    floats = map(lambda f: \"%.4f\" % f, percentiles)\n",
    "    out = [change_name(name)]\n",
    "    out.extend(floats)\n",
    "    print \" & \".join(out) + ' \\\\\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\n",
      "\n",
      "static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n",
      "\t\tunsigned long address, unsigned int *flags, int *nonblocking)\n",
      "{\n",
      "\tstruct mm_struct *mm = vma->vm_mm;\n",
      "\tunsigned int fault_flags = 0;\n",
      "\tint ret;\n",
      "\n",
      "\t/* For mlock, just skip the stack guard page. */\n",
      "\tif ((*flags & FOLL_MLOCK) &&\n",
      "\t\t\t(stack_guard_page_start(vma, address) ||\n",
      "\t\t\t stack_guard_page_end(vma, address + PAGE_SIZE)))\n",
      "\t\treturn -ENOENT;\n",
      "\tif (*flags & FOLL_WRITE)\n",
      "\t\tfault_flags |= FAULT_FLAG_WRITE;\n",
      "\tif (nonblocking)\n",
      "\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n",
      "\tif (*flags & FOLL_NOWAIT)\n",
      "\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n",
      "\n",
      "\tret = handle_mm_fault(mm, vma, address, fault_flags);\n",
      "\tif (ret & VM_FAULT_ERROR) {\n",
      "\t\tif (ret & VM_FAULT_OOM)\n",
      "\t\t\treturn -ENOMEM;\n",
      "\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n",
      "\t\t\treturn *flags & FOLL_HWPOISON ? -EHWPOISON : -EFAULT;\n",
      "\t\tif (ret & VM_FAULT_SIGBUS)\n",
      "\t\t\treturn -EFAULT;\n",
      "\t\tBUG();\n",
      "\t}\n",
      "\n",
      "\tif (tsk) {\n",
      "\t\tif (ret & VM_FAULT_MAJOR)\n",
      "\t\t\ttsk->maj_flt++;\n",
      "\t\telse\n",
      "\t\t\ttsk->min_flt++;\n",
      "\t}\n",
      "\n",
      "\tif (ret & VM_FAULT_RETRY) {\n",
      "\t\tif (nonblocking)\n",
      "\t\t\t*nonblocking = 0;\n",
      "\t\treturn -EBUSY;\n",
      "\t}\n",
      "\n",
      "\t/*\n",
      "\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n",
      "\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n",
      "\t * can thus safely do subsequent page lookups as if they were reads.\n",
      "\t * But only do so when looping for pte_write is futile: in some cases\n",
      "\t * userspace may also be wanting to write to the gotten user page,\n",
      "\t * which a read fault here might prevent (a readonly page might get\n",
      "\t * reCOWed by userspace write).\n",
      "\t */\n",
      "\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n",
      "\t\t*flags &= ~FOLL_WRITE;\n",
      "\treturn 0;\n",
      "------------------------------------------\n",
      "}\n",
      "\n",
      "static int follow_pfn_pte(struct vm_area_struct *vma, unsigned long address,\n",
      "\t\tpte_t *pte, unsigned int flags)\n",
      "{\n",
      "\t/* No page to get reference */\n",
      "\tif (flags & FOLL_GET)\n",
      "\t\treturn -EFAULT;\n",
      "\n",
      "\tif (flags & FOLL_TOUCH) {\n",
      "\t\tpte_t entry = *pte;\n",
      "\n",
      "\t\tif (flags & FOLL_WRITE)\n",
      "\t\t\tentry = pte_mkdirty(entry);\n",
      "\t\tentry = pte_mkyoung(entry);\n",
      "\n",
      "\t\tif (!pte_same(*pte, entry)) {\n",
      "\t\t\tset_pte_at(vma->vm_mm, address, pte, entry);\n",
      "\t\t\tupdate_mmu_cache(vma, address, pte);\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\t/* Proper page table entry exists, but no corresponding struct page */\n",
      "\treturn -EEXIST;\n",
      "------------------------------------------\n",
      "\tif ((flags & FOLL_WRITE) && !pte_write(pte)) {\n",
      "\t\tpte_unmap_unlock(ptep, ptl);\n",
      "\t\treturn NULL;\n",
      "\t}\n",
      "------------------------------------------\n",
      "static struct page *no_page_table(struct vm_area_struct *vma,\n",
      "\t\tunsigned int flags)\n",
      "{\n",
      "\t/*\n",
      "\t * When core dumping an enormous anonymous area that nobody\n",
      "\t * has touched so far, we don't want to allocate unnecessary pages or\n",
      "\t * page tables.  Return error instead of NULL to skip handle_mm_fault,\n",
      "\t * then get_dump_page() will return NULL to leave a hole in the dump.\n",
      "\t * But we can only make this optimization where a hole would surely\n",
      "\t * be zero-filled if handle_mm_fault() actually did handle it.\n",
      "\t */\n",
      "\tif ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))\n",
      "\t\treturn ERR_PTR(-EFAULT);\n",
      "\treturn NULL;\n",
      "}\n",
      "\n",
      "static struct page *follow_page_pte(struct vm_area_struct *vma,\n",
      "\t\tunsigned long address, pmd_t *pmd, unsigned int flags)\n",
      "------------------------------------------\n",
      "#include <linux/kernel.h>\n",
      "#include <linux/errno.h>\n",
      "#include <linux/err.h>\n",
      "#include <linux/spinlock.h>\n",
      "\n",
      "#include <linux/hugetlb.h>\n",
      "#include <linux/mm.h>\n",
      "#include <linux/pagemap.h>\n",
      "#include <linux/rmap.h>\n",
      "#include <linux/swap.h>\n",
      "#include <linux/swapops.h>\n",
      "\n",
      "#include \"internal.h\"\n",
      "\n",
      "/**\n",
      " * follow_page_mask - look up a page descriptor from a user-virtual address\n",
      " * @vma: vm_area_struct mapping @address\n",
      " * @address: virtual address to look up\n",
      " * @flags: flags modifying lookup behaviour\n",
      " * @page_mask: on output, *page_mask is set according to the size of the page\n",
      " *\n",
      " * @flags can have FOLL_ flags set, defined in <linux/mm.h>\n",
      " *\n",
      " * Returns the mapped (struct page *), %NULL if no mapping exists, or\n",
      " * an error pointer if there is a mapping to something not represented\n",
      " * by a page descriptor (see also vm_normal_page()).\n",
      " */\n",
      "struct page *follow_page_mask(struct vm_area_struct *vma,\n",
      "\t\t\t      unsigned long address, unsigned int flags,\n",
      "\t\t\t      unsigned int *page_mask)\n",
      "{\n",
      "\tpgd_t *pgd;\n",
      "\tpud_t *pud;\n",
      "\tpmd_t *pmd;\n",
      "\tpte_t *ptep, pte;\n",
      "\tspinlock_t *ptl;\n",
      "\tstruct page *page;\n",
      "\tstruct mm_struct *mm = vma->vm_mm;\n",
      "\n",
      "\t*page_mask = 0;\n",
      "\n",
      "\tpage = follow_huge_addr(mm, address, flags & FOLL_WRITE);\n",
      "\tif (!IS_ERR(page)) {\n",
      "\t\tBUG_ON(flags & FOLL_GET);\n",
      "\t\tgoto out;\n",
      "\t}\n",
      "\n",
      "\tpage = NULL;\n",
      "\tpgd = pgd_offset(mm, address);\n",
      "\tif (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))\n",
      "\t\tgoto no_page_table;\n",
      "\n",
      "\tpud = pud_offset(pgd, address);\n",
      "\tif (pud_none(*pud))\n",
      "\t\tgoto no_page_table;\n",
      "\tif (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {\n",
      "\t\tif (flags & FOLL_GET)\n",
      "\t\t\tgoto out;\n",
      "\t\tpage = follow_huge_pud(mm, address, pud, flags & FOLL_WRITE);\n",
      "\t\tgoto out;\n",
      "\t}\n",
      "\tif (unlikely(pud_bad(*pud)))\n",
      "\t\tgoto no_page_table;\n",
      "\n",
      "\tpmd = pmd_offset(pud, address);\n",
      "\tif (pmd_none(*pmd))\n",
      "\t\tgoto no_page_table;\n",
      "\tif (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {\n",
      "\t\tpage = follow_huge_pmd(mm, address, pmd, flags & FOLL_WRITE);\n",
      "\t\tif (flags & FOLL_GET) {\n",
      "\t\t\t/*\n",
      "\t\t\t * Refcount on tail pages are not well-defined and\n",
      "\t\t\t * shouldn't be taken. The caller should handle a NULL\n",
      "\t\t\t * return when trying to follow tail pages.\n",
      "\t\t\t */\n",
      "\t\t\tif (PageHead(page))\n",
      "\t\t\t\tget_page(page);\n",
      "\t\t\telse {\n",
      "\t\t\t\tpage = NULL;\n",
      "\t\t\t\tgoto out;\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t\tgoto out;\n",
      "\t}\n",
      "\tif ((flags & FOLL_NUMA) && pmd_numa(*pmd))\n",
      "\t\tgoto no_page_table;\n",
      "\tif (pmd_trans_huge(*pmd)) {\n",
      "\t\tif (flags & FOLL_SPLIT) {\n",
      "\t\t\tsplit_huge_page_pmd(vma, address, pmd);\n",
      "\t\t\tgoto split_fallthrough;\n",
      "\t\t}\n",
      "\t\tptl = pmd_lock(mm, pmd);\n",
      "\t\tif (likely(pmd_trans_huge(*pmd))) {\n",
      "\t\t\tif (unlikely(pmd_trans_splitting(*pmd))) {\n",
      "\t\t\t\tspin_unlock(ptl);\n",
      "\t\t\t\twait_split_huge_page(vma->anon_vma, pmd);\n",
      "\t\t\t} else {\n",
      "\t\t\t\tpage = follow_trans_huge_pmd(vma, address,\n",
      "\t\t\t\t\t\t\t     pmd, flags);\n",
      "\t\t\t\tspin_unlock(ptl);\n",
      "\t\t\t\t*page_mask = HPAGE_PMD_NR - 1;\n",
      "\t\t\t\tgoto out;\n",
      "\t\t\t}\n",
      "\t\t} else\n",
      "\t\t\tspin_unlock(ptl);\n",
      "\t\t/* fall through */\n",
      "\t}\n",
      "split_fallthrough:\n",
      "\tif (unlikely(pmd_bad(*pmd)))\n",
      "\t\tgoto no_page_table;\n",
      "\n",
      "\tptep = pte_offset_map_lock(mm, pmd, address, &ptl);\n",
      "\n",
      "\tpte = *ptep;\n",
      "\tif (!pte_present(pte)) {\n",
      "\t\tswp_entry_t entry;\n",
      "\t\t/*\n",
      "\t\t * KSM's break_ksm() relies upon recognizing a ksm page\n",
      "\t\t * even while it is being migrated, so for that case we\n",
      "\t\t * need migration_entry_wait().\n",
      "\t\t */\n",
      "\t\tif (likely(!(flags & FOLL_MIGRATION)))\n",
      "\t\t\tgoto no_page;\n",
      "\t\tif (pte_none(pte) || pte_file(pte))\n",
      "\t\t\tgoto no_page;\n",
      "\t\tentry = pte_to_swp_entry(pte);\n",
      "\t\tif (!is_migration_entry(entry))\n",
      "\t\t\tgoto no_page;\n",
      "\t\tpte_unmap_unlock(ptep, ptl);\n",
      "\t\tmigration_entry_wait(mm, pmd, address);\n",
      "\t\tgoto split_fallthrough;\n",
      "\t}\n",
      "\tif ((flags & FOLL_NUMA) && pte_numa(pte))\n",
      "\t\tgoto no_page;\n",
      "\tif ((flags & FOLL_WRITE) && !pte_write(pte))\n",
      "\t\tgoto unlock;\n",
      "\n",
      "\tpage = vm_normal_page(vma, address, pte);\n",
      "\tif (unlikely(!page)) {\n",
      "\t\tif ((flags & FOLL_DUMP) ||\n",
      "\t\t    !is_zero_pfn(pte_pfn(pte)))\n",
      "\t\t\tgoto bad_page;\n",
      "\t\tpage = pte_page(pte);\n",
      "\t}\n",
      "\n",
      "\tif (flags & FOLL_GET)\n",
      "\t\tget_page_foll(page);\n",
      "\tif (flags & FOLL_TOUCH) {\n",
      "\t\tif ((flags & FOLL_WRITE) &&\n",
      "\t\t    !pte_dirty(pte) && !PageDirty(page))\n",
      "\t\t\tset_page_dirty(page);\n",
      "\t\t/*\n",
      "\t\t * pte_mkyoung() would be more correct here, but atomic care\n",
      "\t\t * is needed to avoid losing the dirty bit: it is easier to use\n",
      "\t\t * mark_page_accessed().\n",
      "\t\t */\n",
      "\t\tmark_page_accessed(page);\n",
      "\t}\n",
      "\tif ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {\n",
      "\t\t/*\n",
      "\t\t * The preliminary mapping check is mainly to avoid the\n",
      "\t\t * pointless overhead of lock_page on the ZERO_PAGE\n",
      "\t\t * which might bounce very badly if there is contention.\n",
      "\t\t *\n",
      "\t\t * If the page is already locked, we don't need to\n",
      "\t\t * handle it now - vmscan will handle it later if and\n",
      "\t\t * when it attempts to reclaim the page.\n",
      "\t\t */\n",
      "\t\tif (page->mapping && trylock_page(page)) {\n",
      "\t\t\tlru_add_drain();  /* push cached pages to LRU */\n",
      "\t\t\t/*\n",
      "\t\t\t * Because we lock page here, and migration is\n",
      "\t\t\t * blocked by the pte's page reference, and we\n",
      "\t\t\t * know the page is still mapped, we don't even\n",
      "\t\t\t * need to check for file-cache page truncation.\n",
      "\t\t\t */\n",
      "\t\t\tmlock_vma_page(page);\n",
      "\t\t\tunlock_page(page);\n",
      "\t\t}\n",
      "\t}\n",
      "unlock:\n",
      "\tpte_unmap_unlock(ptep, ptl);\n",
      "out:\n",
      "\treturn page;\n",
      "\n",
      "bad_page:\n",
      "\tpte_unmap_unlock(ptep, ptl);\n",
      "\treturn ERR_PTR(-EFAULT);\n",
      "\n",
      "no_page:\n",
      "\tpte_unmap_unlock(ptep, ptl);\n",
      "\tif (!pte_none(pte))\n",
      "\t\treturn page;\n",
      "\n",
      "no_page_table:\n",
      "\t/*\n",
      "\t * When core dumping an enormous anonymous area that nobody\n",
      "\t * has touched so far, we don't want to allocate unnecessary pages or\n",
      "\t * page tables.  Return error instead of NULL to skip handle_mm_fault,\n",
      "\t * then get_dump_page() will return NULL to leave a hole in the dump.\n",
      "\t * But we can only make this optimization where a hole would surely\n",
      "\t * be zero-filled if handle_mm_fault() actually did handle it.\n",
      "\t */\n",
      "\tif ((flags & FOLL_DUMP) &&\n",
      "\t    (!vma->vm_ops || !vma->vm_ops->fault))\n",
      "\t\treturn ERR_PTR(-EFAULT);\n",
      "\treturn page;\n",
      "}\n",
      "\n",
      "static inline int stack_guard_page(struct vm_area_struct *vma, unsigned long addr)\n",
      "{\n",
      "\treturn stack_guard_page_start(vma, addr) ||\n",
      "\t       stack_guard_page_end(vma, addr+PAGE_SIZE);\n",
      "}\n",
      "\n",
      "/**\n",
      " * __get_user_pages() - pin user pages in memory\n",
      " * @tsk:\ttask_struct of target task\n",
      " * @mm:\t\tmm_struct of target mm\n",
      " * @start:\tstarting user address\n",
      " * @nr_pages:\tnumber of pages from start to pin\n",
      " * @gup_flags:\tflags modifying pin behaviour\n",
      " * @pages:\tarray that receives pointers to the pages pinned.\n",
      " *\t\tShould be at least nr_pages long. Or NULL, if caller\n",
      " *\t\tonly intends to ensure the pages are faulted in.\n",
      " * @vmas:\tarray of pointers to vmas corresponding to each page.\n",
      " *\t\tOr NULL if the caller does not require them.\n",
      " * @nonblocking: whether waiting for disk IO or mmap_sem contention\n",
      " *\n",
      " * Returns number of pages pinned. This may be fewer than the number\n",
      " * requested. If nr_pages is 0 or negative, returns 0. If no pages\n",
      " * were pinned, returns -errno. Each page returned must be released\n",
      " * with a put_page() call when it is finished with. vmas will only\n",
      " * remain valid while mmap_sem is held.\n",
      " *\n",
      " * Must be called with mmap_sem held for read or write.\n",
      " *\n",
      " * __get_user_pages walks a process's page tables and takes a reference to\n",
      " * each struct page that each user address corresponds to at a given\n",
      " * instant. That is, it takes the page that would be accessed if a user\n",
      " * thread accesses the given user virtual address at that instant.\n",
      " *\n",
      " * This does not guarantee that the page exists in the user mappings when\n",
      " * __get_user_pages returns, and there may even be a completely different\n",
      " * page there in some cases (eg. if mmapped pagecache has been invalidated\n",
      " * and subsequently re faulted). However it does guarantee that the page\n",
      " * won't be freed completely. And mostly callers simply care that the page\n",
      " * contains data that was valid *at some point in time*. Typically, an IO\n",
      " * or similar operation cannot guarantee anything stronger anyway because\n",
      " * locks can't be held over the syscall boundary.\n",
      " *\n",
      " * If @gup_flags & FOLL_WRITE == 0, the page must not be written to. If\n",
      " * the page is written to, set_page_dirty (or set_page_dirty_lock, as\n",
      " * appropriate) must be called after the page is finished with, and\n",
      " * before put_page is called.\n",
      " *\n",
      " * If @nonblocking != NULL, __get_user_pages will not wait for disk IO\n",
      " * or mmap_sem contention, and if waiting is needed to pin all pages,\n",
      " * *@nonblocking will be set to 0.\n",
      " *\n",
      " * In most cases, get_user_pages or get_user_pages_fast should be used\n",
      " * instead of __get_user_pages. __get_user_pages should be used only if\n",
      " * you need some special @gup_flags.\n",
      " */\n",
      "long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n",
      "\t\tunsigned long start, unsigned long nr_pages,\n",
      "\t\tunsigned int gup_flags, struct page **pages,\n",
      "\t\tstruct vm_area_struct **vmas, int *nonblocking)\n",
      "{\n",
      "\tlong i;\n",
      "\tunsigned long vm_flags;\n",
      "\tunsigned int page_mask;\n",
      "\n",
      "\tif (!nr_pages)\n",
      "\t\treturn 0;\n",
      "\n",
      "\tVM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));\n",
      "\n",
      "\t/*\n",
      "\t * If FOLL_FORCE is set then do not force a full fault as the hinting\n",
      "\t * fault information is unrelated to the reference behaviour of a task\n",
      "\t * using the address space\n",
      "\t */\n",
      "\tif (!(gup_flags & FOLL_FORCE))\n",
      "\t\tgup_flags |= FOLL_NUMA;\n",
      "\n",
      "\ti = 0;\n",
      "\n",
      "\tdo {\n",
      "\t\tstruct vm_area_struct *vma;\n",
      "\n",
      "\t\tvma = find_extend_vma(mm, start);\n",
      "\t\tif (!vma && in_gate_area(mm, start)) {\n",
      "\t\t\tunsigned long pg = start & PAGE_MASK;\n",
      "\t\t\tpgd_t *pgd;\n",
      "\t\t\tpud_t *pud;\n",
      "\t\t\tpmd_t *pmd;\n",
      "\t\t\tpte_t *pte;\n",
      "\n",
      "\t\t\t/* user gate pages are read-only */\n",
      "\t\t\tif (gup_flags & FOLL_WRITE)\n",
      "\t\t\t\tgoto efault;\n",
      "\t\t\tif (pg > TASK_SIZE)\n",
      "\t\t\t\tpgd = pgd_offset_k(pg);\n",
      "\t\t\telse\n",
      "\t\t\t\tpgd = pgd_offset_gate(mm, pg);\n",
      "\t\t\tBUG_ON(pgd_none(*pgd));\n",
      "\t\t\tpud = pud_offset(pgd, pg);\n",
      "\t\t\tBUG_ON(pud_none(*pud));\n",
      "\t\t\tpmd = pmd_offset(pud, pg);\n",
      "\t\t\tif (pmd_none(*pmd))\n",
      "\t\t\t\tgoto efault;\n",
      "\t\t\tVM_BUG_ON(pmd_trans_huge(*pmd));\n",
      "\t\t\tpte = pte_offset_map(pmd, pg);\n",
      "\t\t\tif (pte_none(*pte)) {\n",
      "\t\t\t\tpte_unmap(pte);\n",
      "\t\t\t\tgoto efault;\n",
      "\t\t\t}\n",
      "\t\t\tvma = get_gate_vma(mm);\n",
      "\t\t\tif (pages) {\n",
      "\t\t\t\tstruct page *page;\n",
      "\n",
      "\t\t\t\tpage = vm_normal_page(vma, start, *pte);\n",
      "\t\t\t\tif (!page) {\n",
      "\t\t\t\t\tif (!(gup_flags & FOLL_DUMP) &&\n",
      "\t\t\t\t\t     is_zero_pfn(pte_pfn(*pte)))\n",
      "\t\t\t\t\t\tpage = pte_page(*pte);\n",
      "\t\t\t\t\telse {\n",
      "\t\t\t\t\t\tpte_unmap(pte);\n",
      "\t\t\t\t\t\tgoto efault;\n",
      "\t\t\t\t\t}\n",
      "\t\t\t\t}\n",
      "\t\t\t\tpages[i] = page;\n",
      "\t\t\t\tget_page(page);\n",
      "\t\t\t}\n",
      "\t\t\tpte_unmap(pte);\n",
      "\t\t\tpage_mask = 0;\n",
      "\t\t\tgoto next_page;\n",
      "\t\t}\n",
      "\n",
      "\t\tif (!vma)\n",
      "\t\t\tgoto efault;\n",
      "\t\tvm_flags = vma->vm_flags;\n",
      "\t\tif (vm_flags & (VM_IO | VM_PFNMAP))\n",
      "\t\t\tgoto efault;\n",
      "\n",
      "\t\tif (gup_flags & FOLL_WRITE) {\n",
      "\t\t\tif (!(vm_flags & VM_WRITE)) {\n",
      "\t\t\t\tif (!(gup_flags & FOLL_FORCE))\n",
      "\t\t\t\t\tgoto efault;\n",
      "\t\t\t\t/*\n",
      "\t\t\t\t * We used to let the write,force case do COW\n",
      "\t\t\t\t * in a VM_MAYWRITE VM_SHARED !VM_WRITE vma, so\n",
      "\t\t\t\t * ptrace could set a breakpoint in a read-only\n",
      "\t\t\t\t * mapping of an executable, without corrupting\n",
      "\t\t\t\t * the file (yet only when that file had been\n",
      "\t\t\t\t * opened for writing!).  Anon pages in shared\n",
      "\t\t\t\t * mappings are surprising: now just reject it.\n",
      "\t\t\t\t */\n",
      "\t\t\t\tif (!is_cow_mapping(vm_flags)) {\n",
      "\t\t\t\t\tWARN_ON_ONCE(vm_flags & VM_MAYWRITE);\n",
      "\t\t\t\t\tgoto efault;\n",
      "\t\t\t\t}\n",
      "\t\t\t}\n",
      "\t\t} else {\n",
      "\t\t\tif (!(vm_flags & VM_READ)) {\n",
      "\t\t\t\tif (!(gup_flags & FOLL_FORCE))\n",
      "\t\t\t\t\tgoto efault;\n",
      "\t\t\t\t/*\n",
      "\t\t\t\t * Is there actually any vma we can reach here\n",
      "\t\t\t\t * which does not have VM_MAYREAD set?\n",
      "\t\t\t\t */\n",
      "\t\t\t\tif (!(vm_flags & VM_MAYREAD))\n",
      "\t\t\t\t\tgoto efault;\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\n",
      "\t\tif (is_vm_hugetlb_page(vma)) {\n",
      "\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,\n",
      "\t\t\t\t\t&start, &nr_pages, i, gup_flags);\n",
      "\t\t\tcontinue;\n",
      "\t\t}\n",
      "\n",
      "\t\tdo {\n",
      "\t\t\tstruct page *page;\n",
      "\t\t\tunsigned int foll_flags = gup_flags;\n",
      "\t\t\tunsigned int page_increm;\n",
      "\n",
      "\t\t\t/*\n",
      "\t\t\t * If we have a pending SIGKILL, don't keep faulting\n",
      "\t\t\t * pages and potentially allocating memory.\n",
      "\t\t\t */\n",
      "\t\t\tif (unlikely(fatal_signal_pending(current)))\n",
      "\t\t\t\treturn i ? i : -ERESTARTSYS;\n",
      "\n",
      "\t\t\tcond_resched();\n",
      "\t\t\twhile (!(page = follow_page_mask(vma, start,\n",
      "\t\t\t\t\t\tfoll_flags, &page_mask))) {\n",
      "\t\t\t\tint ret;\n",
      "\t\t\t\tunsigned int fault_flags = 0;\n",
      "\n",
      "\t\t\t\t/* For mlock, just skip the stack guard page. */\n",
      "\t\t\t\tif (foll_flags & FOLL_MLOCK) {\n",
      "\t\t\t\t\tif (stack_guard_page(vma, start))\n",
      "\t\t\t\t\t\tgoto next_page;\n",
      "\t\t\t\t}\n",
      "\t\t\t\tif (foll_flags & FOLL_WRITE)\n",
      "\t\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;\n",
      "\t\t\t\tif (nonblocking)\n",
      "\t\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n",
      "\t\t\t\tif (foll_flags & FOLL_NOWAIT)\n",
      "\t\t\t\t\tfault_flags |= (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT);\n",
      "\n",
      "\t\t\t\tret = handle_mm_fault(mm, vma, start,\n",
      "\t\t\t\t\t\t\tfault_flags);\n",
      "\n",
      "\t\t\t\tif (ret & VM_FAULT_ERROR) {\n",
      "\t\t\t\t\tif (ret & VM_FAULT_OOM)\n",
      "\t\t\t\t\t\treturn i ? i : -ENOMEM;\n",
      "\t\t\t\t\tif (ret & (VM_FAULT_HWPOISON |\n",
      "\t\t\t\t\t\t   VM_FAULT_HWPOISON_LARGE)) {\n",
      "\t\t\t\t\t\tif (i)\n",
      "\t\t\t\t\t\t\treturn i;\n",
      "\t\t\t\t\t\telse if (gup_flags & FOLL_HWPOISON)\n",
      "\t\t\t\t\t\t\treturn -EHWPOISON;\n",
      "\t\t\t\t\t\telse\n",
      "\t\t\t\t\t\t\treturn -EFAULT;\n",
      "\t\t\t\t\t}\n",
      "\t\t\t\t\tif (ret & VM_FAULT_SIGBUS)\n",
      "\t\t\t\t\t\tgoto efault;\n",
      "\t\t\t\t\tBUG();\n",
      "\t\t\t\t}\n",
      "\n",
      "\t\t\t\tif (tsk) {\n",
      "\t\t\t\t\tif (ret & VM_FAULT_MAJOR)\n",
      "\t\t\t\t\t\ttsk->maj_flt++;\n",
      "\t\t\t\t\telse\n",
      "\t\t\t\t\t\ttsk->min_flt++;\n",
      "\t\t\t\t}\n",
      "\n",
      "\t\t\t\tif (ret & VM_FAULT_RETRY) {\n",
      "\t\t\t\t\tif (nonblocking)\n",
      "\t\t\t\t\t\t*nonblocking = 0;\n",
      "\t\t\t\t\treturn i;\n",
      "\t\t\t\t}\n",
      "\n",
      "\t\t\t\t/*\n",
      "\t\t\t\t * The VM_FAULT_WRITE bit tells us that\n",
      "\t\t\t\t * do_wp_page has broken COW when necessary,\n",
      "\t\t\t\t * even if maybe_mkwrite decided not to set\n",
      "\t\t\t\t * pte_write. We can thus safely do subsequent\n",
      "\t\t\t\t * page lookups as if they were reads. But only\n",
      "\t\t\t\t * do so when looping for pte_write is futile:\n",
      "\t\t\t\t * in some cases userspace may also be wanting\n",
      "\t\t\t\t * to write to the gotten user page, which a\n",
      "\t\t\t\t * read fault here might prevent (a readonly\n",
      "\t\t\t\t * page might get reCOWed by userspace write).\n",
      "\t\t\t\t */\n",
      "\t\t\t\tif ((ret & VM_FAULT_WRITE) &&\n",
      "\t\t\t\t    !(vma->vm_flags & VM_WRITE))\n",
      "\t\t\t\t\tfoll_flags &= ~FOLL_WRITE;\n",
      "\n",
      "\t\t\t\tcond_resched();\n",
      "\t\t\t}\n",
      "\t\t\tif (IS_ERR(page))\n",
      "\t\t\t\treturn i ? i : PTR_ERR(page);\n",
      "\t\t\tif (pages) {\n",
      "\t\t\t\tpages[i] = page;\n",
      "\n",
      "\t\t\t\tflush_anon_page(vma, page, start);\n",
      "\t\t\t\tflush_dcache_page(page);\n",
      "\t\t\t\tpage_mask = 0;\n",
      "\t\t\t}\n",
      "next_page:\n",
      "\t\t\tif (vmas) {\n",
      "\t\t\t\tvmas[i] = vma;\n",
      "\t\t\t\tpage_mask = 0;\n",
      "\t\t\t}\n",
      "\t\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);\n",
      "\t\t\tif (page_increm > nr_pages)\n",
      "\t\t\t\tpage_increm = nr_pages;\n",
      "\t\t\ti += page_increm;\n",
      "\t\t\tstart += page_increm * PAGE_SIZE;\n",
      "\t\t\tnr_pages -= page_increm;\n",
      "\t\t} while (nr_pages && start < vma->vm_end);\n",
      "\t} while (nr_pages);\n",
      "\treturn i;\n",
      "efault:\n",
      "\treturn i ? : -EFAULT;\n",
      "}\n",
      "EXPORT_SYMBOL(__get_user_pages);\n",
      "\n",
      "/*\n",
      " * fixup_user_fault() - manually resolve a user page fault\n",
      " * @tsk:\tthe task_struct to use for page fault accounting, or\n",
      " *\t\tNULL if faults are not to be recorded.\n",
      " * @mm:\t\tmm_struct of target mm\n",
      " * @address:\tuser address\n",
      " * @fault_flags:flags to pass down to handle_mm_fault()\n",
      " *\n",
      " * This is meant to be called in the specific scenario where for locking reasons\n",
      " * we try to access user memory in atomic context (within a pagefault_disable()\n",
      " * section), this returns -EFAULT, and we want to resolve the user fault before\n",
      " * trying again.\n",
      " *\n",
      " * Typically this is meant to be used by the futex code.\n",
      " *\n",
      " * The main difference with get_user_pages() is that this function will\n",
      " * unconditionally call handle_mm_fault() which will in turn perform all the\n",
      " * necessary SW fixup of the dirty and young bits in the PTE, while\n",
      " * handle_mm_fault() only guarantees to update these in the struct page.\n",
      " *\n",
      " * This is important for some architectures where those bits also gate the\n",
      " * access permission to the page because they are maintained in software.  On\n",
      " * such architectures, gup() will not be enough to make a subsequent access\n",
      " * succeed.\n",
      " *\n",
      " * This should be called with the mm_sem held for read.\n",
      " */\n",
      "int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\n",
      "\t\t     unsigned long address, unsigned int fault_flags)\n",
      "{\n",
      "\tstruct vm_area_struct *vma;\n",
      "\tvm_flags_t vm_flags;\n",
      "\tint ret;\n",
      "\n",
      "\tvma = find_extend_vma(mm, address);\n",
      "\tif (!vma || address < vma->vm_start)\n",
      "\t\treturn -EFAULT;\n",
      "\n",
      "\tvm_flags = (fault_flags & FAULT_FLAG_WRITE) ? VM_WRITE : VM_READ;\n",
      "\tif (!(vm_flags & vma->vm_flags))\n",
      "\t\treturn -EFAULT;\n",
      "\n",
      "\tret = handle_mm_fault(mm, vma, address, fault_flags);\n",
      "\tif (ret & VM_FAULT_ERROR) {\n",
      "\t\tif (ret & VM_FAULT_OOM)\n",
      "\t\t\treturn -ENOMEM;\n",
      "\t\tif (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n",
      "\t\t\treturn -EHWPOISON;\n",
      "\t\tif (ret & VM_FAULT_SIGBUS)\n",
      "\t\t\treturn -EFAULT;\n",
      "\t\tBUG();\n",
      "\t}\n",
      "\tif (tsk) {\n",
      "\t\tif (ret & VM_FAULT_MAJOR)\n",
      "\t\t\ttsk->maj_flt++;\n",
      "\t\telse\n",
      "\t\t\ttsk->min_flt++;\n",
      "\t}\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "/*\n",
      " * get_user_pages() - pin user pages in memory\n",
      " * @tsk:\tthe task_struct to use for page fault accounting, or\n",
      " *\t\tNULL if faults are not to be recorded.\n",
      " * @mm:\t\tmm_struct of target mm\n",
      " * @start:\tstarting user address\n",
      " * @nr_pages:\tnumber of pages from start to pin\n",
      " * @write:\twhether pages will be written to by the caller\n",
      " * @force:\twhether to force access even when user mapping is currently\n",
      " *\t\tprotected (but never forces write access to shared mapping).\n",
      " * @pages:\tarray that receives pointers to the pages pinned.\n",
      " *\t\tShould be at least nr_pages long. Or NULL, if caller\n",
      " *\t\tonly intends to ensure the pages are faulted in.\n",
      " * @vmas:\tarray of pointers to vmas corresponding to each page.\n",
      " *\t\tOr NULL if the caller does not require them.\n",
      " *\n",
      " * Returns number of pages pinned. This may be fewer than the number\n",
      " * requested. If nr_pages is 0 or negative, returns 0. If no pages\n",
      " * were pinned, returns -errno. Each page returned must be released\n",
      " * with a put_page() call when it is finished with. vmas will only\n",
      " * remain valid while mmap_sem is held.\n",
      " *\n",
      " * Must be called with mmap_sem held for read or write.\n",
      " *\n",
      " * get_user_pages walks a process's page tables and takes a reference to\n",
      " * each struct page that each user address corresponds to at a given\n",
      " * instant. That is, it takes the page that would be accessed if a user\n",
      " * thread accesses the given user virtual address at that instant.\n",
      " *\n",
      " * This does not guarantee that the page exists in the user mappings when\n",
      " * get_user_pages returns, and there may even be a completely different\n",
      " * page there in some cases (eg. if mmapped pagecache has been invalidated\n",
      " * and subsequently re faulted). However it does guarantee that the page\n",
      " * won't be freed completely. And mostly callers simply care that the page\n",
      " * contains data that was valid *at some point in time*. Typically, an IO\n",
      " * or similar operation cannot guarantee anything stronger anyway because\n",
      " * locks can't be held over the syscall boundary.\n",
      " *\n",
      " * If write=0, the page must not be written to. If the page is written to,\n",
      " * set_page_dirty (or set_page_dirty_lock, as appropriate) must be called\n",
      " * after the page is finished with, and before put_page is called.\n",
      " *\n",
      " * get_user_pages is typically used for fewer-copy IO operations, to get a\n",
      " * handle on the memory by some means other than accesses via the user virtual\n",
      " * addresses. The pages may be submitted for DMA to devices or accessed via\n",
      " * their kernel linear mapping (via the kmap APIs). Care should be taken to\n",
      " * use the correct cache flushing APIs.\n",
      " *\n",
      " * See also get_user_pages_fast, for performance critical applications.\n",
      " */\n",
      "long get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n",
      "\t\tunsigned long start, unsigned long nr_pages, int write,\n",
      "\t\tint force, struct page **pages, struct vm_area_struct **vmas)\n",
      "{\n",
      "\tint flags = FOLL_TOUCH;\n",
      "\n",
      "\tif (pages)\n",
      "\t\tflags |= FOLL_GET;\n",
      "\tif (write)\n",
      "\t\tflags |= FOLL_WRITE;\n",
      "\tif (force)\n",
      "\t\tflags |= FOLL_FORCE;\n",
      "\n",
      "\treturn __get_user_pages(tsk, mm, start, nr_pages, flags, pages, vmas,\n",
      "\t\t\t\tNULL);\n",
      "}\n",
      "EXPORT_SYMBOL(get_user_pages);\n",
      "\n",
      "/**\n",
      " * get_dump_page() - pin user page in memory while writing it to core dump\n",
      " * @addr: user address\n",
      " *\n",
      " * Returns struct page pointer of user page pinned for dump,\n",
      " * to be freed afterwards by page_cache_release() or put_page().\n",
      " *\n",
      " * Returns NULL on any kind of failure - a hole must then be inserted into\n",
      " * the corefile, to preserve alignment with its headers; and also returns\n",
      " * NULL wherever the ZERO_PAGE, or an anonymous pte_none, has been found -\n",
      " * allowing a hole to be left in the corefile to save diskspace.\n",
      " *\n",
      " * Called without mmap_sem, but after all other threads have been killed.\n",
      " */\n",
      "#ifdef CONFIG_ELF_CORE\n",
      "struct page *get_dump_page(unsigned long addr)\n",
      "{\n",
      "\tstruct vm_area_struct *vma;\n",
      "\tstruct page *page;\n",
      "\n",
      "\tif (__get_user_pages(current, current->mm, addr, 1,\n",
      "\t\t\t     FOLL_FORCE | FOLL_DUMP | FOLL_GET, &page, &vma,\n",
      "\t\t\t     NULL) < 1)\n",
      "\t\treturn NULL;\n",
      "\tflush_cache_page(vma, addr, page_to_pfn(page));\n",
      "\treturn page;\n",
      "}\n",
      "#endif /* CONFIG_ELF_CORE */\n",
      "\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for key, _ in buggy_dict.iteritems():\n",
    "    for line in (eval(buggy_dict[key]['patch']['content'])):\n",
    "        print line\n",
    "    print '------------------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
